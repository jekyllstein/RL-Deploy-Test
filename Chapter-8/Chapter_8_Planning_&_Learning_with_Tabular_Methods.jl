### A Pluto.jl notebook ###
# v0.19.40

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local iv = try Base.loaded_modules[Base.PkgId(Base.UUID("6e696c72-6542-2067-7265-42206c756150"), "AbstractPlutoDingetjes")].Bonds.initial_value catch; b -> missing; end
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : iv(el)
        el
    end
end

# ╔═╡ 0fff8e1b-d0c2-49b8-93b4-8d1615c26690
begin
	using Statistics, PlutoPlotly, Random, StatsBase, PlutoUI, HypertextLiteral, DataStructures, StaticArrays, Transducers
	TableOfContents()
end

# ╔═╡ 516234a8-2748-11ed-35df-432eebaa5162
md"""
# Chapter 8 - Planning and Learning with Tabular Methods

So far we have seen example problems where we have a full model of the environment in the form of the probability transition function as well as environments where we can only sample trajectories.  In this chapter we will integrate the techniques that are *model-based* and *model-free* and show how they can be intermixed.

## 8.1 Models and Planning

A *model* is anything that an agent can use to predict the environment.  If the model provides a full description of all possible transitions it is called a *distribution model* vs a *sample model* that can only generate one of those possibilities according to the correct probability distribution.  In dynamic programming, we used a distribution model while for certain example problems such as blackjack we only had a sample model.

A model can be used the create a *simulated experience* of the environment such as a trajectory.  The common thread across all the techniques is the computation of the value function to improve a policy and using some update process to compute the value function for example from the data collected in simulated experience.  For the learning methods considered so far, we have assumed that the data collected from trajectories is generated by the environment itself while in planning methods this experience would come instead from a model.  However the learning techniques largely still apply to planning techniques as well since the nature of the data is the same.  Consider a planning method analogous to Q-learning called *random-sample one-step tabular Q-planning*.  This technique applies the Q-learning update to a transition sapmled from a model.  Instead of interacting with the environment in an episode or continuing task, this technique simply selects a state action pair at random and observes the transition.  Just like with Q-learning, this method converges to the optimal policy under the assumption that all state-action pairs are visited an infinite number of times but the policy will only be optimal for the model of the environment.

Performing updates on single transitions highlights another theme of planning methods which don't necessarily involve exaustive solutions to the whole environment.  We can direct the method to specific states of interest which may be important for problems with very large state spaces.

## 8.2 Dyna: Integrated Planning, Acting, and Learning

A planning agent can use real experience in at least two ways: 1) it can be used to improve the model to make it a better match for the real environment (*model-learning*) and 2) it can be used directly to improve the value function using the previous learning methods (*direct reinforcement learning*).  If a better model is then used to improve the value function this is also called *indirect reinforcement learning*.  

Indirect methods can make better use of a limited amount of experience, but direct methods are much simpler and are not affected by the biases in the design of the model.  Dyna-Q includes all the processes of planning, acting, model-learning, and direct RL.  The planning method is the random-sample one-step tabular Q-planning described above.  The direct RL method is one-step tabular Q-learning.  The model-learning mdethod is also table-based and assumes the environment is deterministic.  After each transition $S_t,A_t \longrightarrow R_{t+1},S_{t+1}$, the model records in its table entry for $S_t,A_t$ the prediction that $R_{t+1},S_{t+1}$ will deterministically follow.  Thus if the model is queried with a state-action pair that has been experienced before, it simply returns the last-observed next state and next reward as its prediction.

During planning, the Q-planning algorithm randomly samples only from state-action pairs that have previously been experienced, so the model is never queried with a pair about which it has no information.  The learning and planning portions of the algorithm are connected in that they use the same type of update.  The only difference is the source of the experience used.

The collection of real experience and planning could occur simultaneously in these agents, but for a serial implementation it is assumed that the acting, model-learning, and direct RL processes are very fast while the planning process is the model computation-intensive.  Let us assume that after each step of acting, model-learning, and direct RL there is time for $n$ iterations of the Q-planning algorithm.  Without the model update and the $n$ loop planning step, this algorithm is identical to one-step tabular Q-learning.  An example implementation is below along with an example applying it to a maze environment.
"""

# ╔═╡ 65818e67-c146-4686-a9aa-d0859ef662fb
md"""
### Example 8.1: Dyna Maze
"""

# ╔═╡ d5ac7c6f-9636-46d9-806f-34d6c8e4d4d5
md"""
Consider the simple maze shown above where the agent can take four actions which take it deterministically into the neighboring square unless it is blocked by a wall represented by a W in the diagram.  In this case the agent remains in its original state.  Reward is zero on all transitions except those into the goal state, on which it is +1.  After reaching the goal the agent returns to the start to begin a new episode.  This is a discounted episodic task with $\gamma = 0.95$
"""

# ╔═╡ 76489f21-677e-4c25-beaa-afaf2244cd94
md"""
#### Figure 8.2

Learning curves for a Dyna-Q agent in the maze
"""

# ╔═╡ 27d12c1c-ddb0-4bc1-af51-3388ff806705
md"""
#### Figure 8.3:

Policies found by planning and nonplanning Dyna-Q agents after episode $(@bind num_episodes_8_3 NumberField(1:100, default = 2)). The arrows indicate the greedy action with respect to the learned state-action value function.  Without planning, the only states with a policy update are those within n episodes of the goal where n is the number of episodes experienced so far in training.  For the planning agent after 1 episode, at least one of the values will be updated so during future planning steps, that information can propagate through the other states through bootstrapping since we are free to sample transitions from states into the ones that already have a value update.
"""

# ╔═╡ cd79ce14-14a1-43c6-93e0-b4a786f7f9fb
md"""
Because planning proceeds incrementally, it is trivial to intermix planning and acting.  So the learning from the model can inform the interaction with the environment in parallel and then any information from the environment can be used to update the model whenever it is received.
"""

# ╔═╡ e0cc1ca1-595d-44e2-8612-261df9e2d327
md"""
> ### *Exercise 8.1* 
> The nonplanning method looks particularly poor in Figure 8.3 because it is a one-step method; a method using multi-step bootstrapping would do better. Do you think one of the multi-step bootstrapping methods from Chapter 7 could do as well as the Dyna method? Explain why or why not.

For the n = 50 agent, it can learn a policy that covers nearly the entire maze during the second episode.  In the extreme case of a multistep method we would attempt to solve the maze using monte carlo sampling in which case after a single episode we would have action/value updates for every state visited along the random trajectory.  However, these action/value estimates would be high variance estimates of the randomly initialized policy.  In contrast, the Dyna method after one random episode has observed most or all of the transitions in the maze.  During the long planning phase its Q updates would actually be able to converge to the optimal policy given a large enough n using bootstrapping and the single reward from reaching the goal.  As long as something is sampled close to the goal that information will propagate through to the rest of the states and each update is simultaneously improving the ϵ-greedy policy.  With multi-step bootstrapping we can extend the updates back along the trajectory a certain distance, but in the extreme case we just sample values from the random policy without any bootstrapping or we bootstrap to a limited degree close to the goal and still have the lack of information further away from it.  Since this environment is deterministic, having the sample transitions is equivalent to having a full model of the environment.  This could be used explicitely with value iteration to obtain the Q function as well.  The planning step is effectively performing this computation but only using the known transitions and over time is focused only on the states visited by the optimal or nearly optimal policy.  No n-step method can take advantage of previously observed transitions this well, but it should be noted that Dyna-Q only works this well if the environment is deterministic and is taking advantage of the fact that we know our model is correct for the transitions already observed.  
"""

# ╔═╡ c71ae2cb-8492-4bbb-8fff-3f7c6d096563
maze8_1 = create_maze(9, 6, GridPoint(1, 4), GridPoint(9, 6), obstacles = Set([GridPoint(p...) for p in [(3, 3), (3, 4), (3, 5), (6, 2), (8, 4), (8, 5), (8, 6)]]))

# ╔═╡ 4f4551fe-54a9-4186-ab8f-3535dc2bf4c5
md"""
## 8.3 When the Model Is Wrong

### Example 8.2: Blocking Maze
"""

# ╔═╡ 870b7e41-7d1f-4af3-a145-8952a7fc8d78
maze8_2 = create_maze(9, 6, GridPoint(1, 4), GridPoint(9, 6), obstacles = Set([GridPoint(x, 3) for x in 1:8]))

# ╔═╡ 24efe9b4-9308-4ad1-8ef0-69f6f93407c0
md"""
> *Exercise 8.2* Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?

For the second phase, the maze changed in both cases so the exploration reward bonus in Dyna-Q+ encourages the policy to attempt different actions that have not been visited recently which would result in model updates that reflect the new environment.  For the first phase where the model is accurate, this task may benefit from larger initial exploration than the ϵ of 0.1 provides.  In that case the Dyna-Q+ reward simply acts like if we had a larger ϵ in the first place which may result in faster learning.
"""

# ╔═╡ 26fe0c28-8f0f-4cff-87fb-76f04fce1be1
md"""
> *Exercise 8.3* Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?

As long as the environment isn't changing, no matter how small ϵ is both algorithms will converge to the optimal policy.  After more steps the difficiencies of each algorithm will diminish and they will converge to similar performance.
	"""

# ╔═╡ 340ba72b-172a-4d92-99b2-17687ab511c7
md"""
> *Exercise 8.4 (programming)* The exploration bonus described above actually changes the estimated values of states and actions. Is this necessary? Suppose the bonus $\kappa \sqrt{\tau}$ was used not in updates, but solely in action selection. That is, suppose the action selected was always that for which $Q(S_t,a) + \kappa \sqrt{\tau(S_t, a)}$ was maximal. Carry out a gridworld experiment that tests and illustrates the strengths and weaknesses of this alternate approach.
"""

# ╔═╡ 01f4268e-e947-4057-94a4-19d757be266d
# need to make a maze environment which has an internal state which tracks how many steps have been simulated and then can alter the maze based on that.  Also need to implement Dyna-Q+ which involves augmenting the history with the time since last visited.  Curious about connection with the benefits of the "simulation" steps here with the generalized policy iteration method where you wait until the action/value function has converged with interative updates without actually changing the policy.  Those updates should apply to all states and benefit from fully using the existing experience.

# ╔═╡ d00014fa-1539-4f42-ba63-15c7c9fecfde
function tabular_dynaQplus(env, sinit, sterm, states::AbstractVector{S}, actions::AbstractVector{A}, α, γ, κ, n, maxepisodes; qinit=0.0, rinit = 0.0) where {S, A}
	history = Dict(s => Vector{A}() for s in states) #save a record of all visited states and actions taken
	history_times = Dict{Tuple{S, A}, Int64}()
	Q = Dict(s => fill(qinit, length(actions)) for s in states)
	model = Dict((s, a) => (s, rinit) for s in states for a in actions)
	aidx_lookup = Dict(zip(actions, 1:length(actions)))
	steps = zeros(maxepisodes)
	τ = 1
	
	steprewards = Vector{Float64}()
	for i in 1:maxepisodes
		s = sinit
		while s != sterm
			aidx = ϵ_greedy(s, Q)
			a = actions[aidx]
			push!(history[s], a)
			history_times[(s, a)] = τ
			(s′, r) = env(s, a, τ)
			push!(steprewards, r)
			Q[s][aidx] += α*(r + γ*maximum(Q[s′]) - Q[s][aidx])
			model[(s, a)] = (s′, r)
			s = s′
			#planning updates
			for j in 1:n
				validstates = filter(s -> !isempty(history[s]), keys(history))
				s_sim = rand(validstates)
				a_sim = rand(history[s_sim])
				aidx_sim = aidx_lookup[a_sim]
				(s_sim′, r_sim) = model[(s_sim, a_sim)]
				Q[s_sim][aidx_sim] += α*(r_sim + κ*sqrt(τ - history_times[(s_sim, a_sim)]) + γ*maximum(Q[s_sim′]) - Q[s_sim][aidx_sim])
			end
			steps[i] += 1
			τ += 1
		end	
	end
	return Q, model, steps, steprewards
end

# ╔═╡ a2dbb1e2-2038-40ee-a2d9-8f5a594dd7a8
function test_dynaQplus()
	results = [[begin
		Random.seed!(seed)
		tabular_dynaQplus(maze8_1.env, maze8_1.start, maze8_1.goal, maze8_1.states, maze8_1.actions, 0.1, 0.95, 0.0001, n, 50; qinit=0.0, rinit = 0.0)
	end
	for n in [0, 5, 50]] for seed in 1:30]
	t1 = scatter(x = 2:50, y = mean(r[1][3] for r in results)[2:end], name = "no planning steps")
	t2 = scatter(x = 2:50, y = mean(r[2][3] for r in results)[2:end], name = "5 planning steps")
	t3 = scatter(x = 2:50, y = mean(r[3][3] for r in results)[2:end], name = "50 planning steps")
	plot([t1, t2, t3], Layout(legend_orientation="h"))
end

# ╔═╡ 95d18377-aef7-468c-8b2c-57f82bc7fe91
test_dynaQplus()

# ╔═╡ 321bdf5a-bff7-4181-986f-d3884ea96d27
md"""
# Dependencies
"""

# ╔═╡ e689df6b-d6f0-4928-9212-a940aa00b0ef
html"""
	<style>
		main {
			margin: 0 auto;
			max-width: min(1200px, 90%);
	    	padding-left: max(10px, 5%);
	    	padding-right: max(10px, 5%);
			font-size: max(10px, min(24px, 2vw));
		}
	</style>
	"""

# ╔═╡ b03087e9-e15d-4563-bdae-4d9ba7d2cec6
md"""
## MDP Types and Functions
"""

# ╔═╡ c1ff1bea-649c-4483-b4be-55134f0e8cb7
function sample_action(π::Matrix{T}, i_s::Integer) where T<:AbstractFloat
	(n, m) = size(π)
	sample(1:n, weights(π[:, i_s]))
end

# ╔═╡ 037f1804-b24e-46e7-b2a8-6747e669db66
makelookup(v::Vector) = Dict(x => i for (i, x) in enumerate(v))

# ╔═╡ c62cc32c-0d29-4ea2-8284-ac4c883df6db
struct MDP_TD{S, A, F<:Function, G<:Function, H<:Function}
	states::Vector{S}
	statelookup::Dict{S, Int64}
	actions::Vector{A}
	actionlookup::Dict{A, Int64}
	state_init::G #function that produces an initial state for an episode
	step::F #function that produces reward and updated state given a state action pair
	isterm::H #function that returns true if the state input is terminal
	function MDP_TD(states::Vector{S}, actions::Vector{A}, state_init::G, step::F, isterm::H) where {S, A, F<:Function, G<:Function, H<:Function}
		statelookup = makelookup(states)
		actionlookup = makelookup(actions)
		new{S, A, F, G, H}(states, statelookup, actions, actionlookup, state_init, step, isterm)
	end
end

# ╔═╡ fb5601b0-06d4-43c4-81a6-23a4a8f29f00
function make_random_policy(mdp::MDP_TD; init::T = 1.0f0) where T <: AbstractFloat
	ones(T, length(mdp.actions), length(mdp.states)) ./ length(mdp.actions)
end

# ╔═╡ 1d97325a-8b9a-438d-a5f9-e17638e64627
initialize_state_value(mdp::MDP_TD; vinit::T = 0.0f0) where T<:AbstractFloat = ones(T, length(mdp.states)) .* vinit

# ╔═╡ 7ae23e8e-d554-4d26-a08a-83dab507af13
initialize_state_action_value(mdp::MDP_TD; qinit::T = 0.0f0) where T<:AbstractFloat = ones(T, length(mdp.actions), length(mdp.states)) .* qinit

# ╔═╡ eb9ba23d-bee5-4bb1-b3e1-fe40d9f681dc
function check_policy(π::Matrix{T}, mdp::MDP_TD) where {T <: AbstractFloat}
#checks to make sure that a policy is defined over the same space as an MDP
	(n, m) = size(π)
	num_actions = length(mdp.actions)
	num_states = length(mdp.states)
	@assert n == num_actions "The policy distribution length $n does not match the number of actions in the mdp of $(num_actions)"
	@assert m == num_states "The policy is defined over $m states which does not match the mdp state count of $num_states"
	return nothing
end

# ╔═╡ 618b5f35-2df5-4ffb-a34f-add542691080
#take a step in the environment from state s using policy π
function takestep(mdp::MDP_TD{S, A, F, G, H}, π::Matrix{T}, s::S) where {S, A, F<:Function, G<:Function, H<:Function, T<:Real}
	i_s = mdp.statelookup[s]
	i_a = sample_action(π, i_s)
	a = mdp.actions[i_a]
	(r, s′) = mdp.step(s, a)
	i_s′ = mdp.statelookup[s′]
	return (i_s, i_s′, r, s′, a, i_a)
end

# ╔═╡ b15f1262-1acf-40e5-87a7-bc4b1b437a42
function runepisode(mdp::MDP_TD{S, A, F, G, H}, π::Matrix{T}; max_steps = Inf) where {S, A, F, G, H, T<:Real}
	states = Vector{S}()
	actions = Vector{A}()
	rewards = Vector{T}()
	s = mdp.state_init()
	step = 1
	sterm = s

	#note that the terminal state will not be added to the state list
	while !mdp.isterm(s) && (step <= max_steps)
		push!(states, s)
		(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
		push!(actions, a)
		push!(rewards, r)
		s = s′
		step += 1
		if mdp.isterm(s′)
			sterm = s′
		end
	end
	return states, actions, rewards, sterm
end

# ╔═╡ 6612f482-2f10-43fa-9b7b-2f0c6a94b8e8
runepisode(mdp::MDP_TD; kwargs...) = runepisode(mdp, make_random_policy(mdp); kwargs...)

# ╔═╡ 94b339bb-6e2d-422f-8043-615e8be9a217
begin
	abstract type CompleteMDP{T<:Real} end
	struct FiniteMDP{T<:Real, S, A} <: CompleteMDP{T} 
		states::Vector{S}
		actions::Vector{A}
		rewards::Vector{T}
		# ptf::Dict{Tuple{S, A}, Matrix{T}}
		ptf::Array{T, 4}
		action_scratch::Vector{T}
		state_scratch::Vector{T}
		reward_scratch::Vector{T}
		state_index::Dict{S, Int64}
		action_index::Dict{A, Int64}
		function FiniteMDP{T, S, A}(states::Vector{S}, actions::Vector{A}, rewards::Vector{T}, ptf::Array{T, 4}) where {T <: Real, S, A}
			new(states, actions, rewards, ptf, Vector{T}(undef, length(actions)), Vector{T}(undef, length(states)+1), Vector{T}(undef, length(rewards)), Dict(zip(states, eachindex(states))), Dict(zip(actions, eachindex(actions))))
		end	
	end
	FiniteMDP(states::Vector{S}, actions::Vector{A}, rewards::Vector{T}, ptf::Array{T, 4}) where {T <: Real, S, A} = FiniteMDP{T, S, A}(states, actions, rewards, ptf)
end

# ╔═╡ 726af565-8905-4409-864f-a5c1b5767e09
#forms a random policy for a generic finite state mdp.  The policy is a matrix where the rows represent actions and the columns represent states.  Each column is a probability distribution of actions over that state.
form_random_policy(mdp::CompleteMDP{T}) where T = ones(T, length(mdp.actions), length(mdp.states)) ./ length(mdp.actions)

# ╔═╡ 33f66659-1a87-4890-9137-dbc7776a19d8
function make_greedy_policy!(π::Matrix{T}, mdp::FiniteMDP{T, S, A}, V::Vector{T}, γ::T) where {T<:Real,S,A}
	for i_s in eachindex(mdp.states)
		maxv = -Inf
		for i_a in eachindex(mdp.actions)
			x = zero(T)
			for i_r in eachindex(mdp.rewards)
				for i_s′ in eachindex(V)
					x += mdp.ptf[i_s′, i_r, i_a, i_s] * (mdp.rewards[i_r] + γ * V[i_s′])
				end
			end
			maxv = max(maxv, x)
			π[i_a, i_s] = x
		end
		π[:, i_s] .= (π[:, i_s] .≈ maxv)
		π[:, i_s] ./= sum(π[:, i_s])
	end
	return π
end

# ╔═╡ 195d2a34-c44c-4088-8ec4-dece3107f16d
function bellman_optimal_value!(V::Vector{T}, mdp::FiniteMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	delt = zero(T)
	@inbounds @fastmath @simd for i_s in eachindex(mdp.states)
		maxvalue = typemin(T)
		@inbounds @fastmath @simd for i_a in eachindex(mdp.actions)
			x = zero(T)
			for (i_r, r) in enumerate(mdp.rewards)
				@inbounds @fastmath @simd for i_s′ in eachindex(V)
					x += mdp.ptf[i_s′, i_r, i_a, i_s] * (r + γ * V[i_s′])
				end
			end
			maxvalue = max(maxvalue, x)
		end
		delt = max(delt, abs(maxvalue - V[i_s]) / (eps(abs(V[i_s])) + abs(V[i_s])))
		V[i_s] = maxvalue
	end
	return delt
end

# ╔═╡ 80affd41-b5e6-4b9c-b827-4e3b39bd7767
function value_iteration_v!(V, θ, mdp, γ, nmax, valuelist)
	nmax <= 0 && return valuelist
	
	#update value function
	delt = bellman_optimal_value!(V, mdp, γ)
	
	#add copy of value function to results list
	push!(valuelist, copy(V))

	#halt when value function is no longer changing
	delt <= θ && return valuelist
	
	value_iteration_v!(V, θ, mdp, γ, nmax - 1, valuelist)	
end

# ╔═╡ 6cf35193-dba5-4f78-a4ac-245dda7a0846
begin_value_iteration_v(mdp::FiniteMDP{T,S,A}, γ::T; Vinit::T = zero(T), kwargs...) where {T<:Real,S,A} = begin_value_iteration_v(mdp, γ, Vinit .* ones(T, size(mdp.ptf, 1)); kwargs...)

# ╔═╡ 96bd8d33-d4e8-45bf-9b75-43e8bda6fa07
function make_ϵ_greedy_policy!(v::AbstractVector{T}, ϵ::T; valid_inds = eachindex(v)) where T <: Real
	vmax = maximum(view(v, valid_inds))
	v .= T.(isapprox.(v, vmax))
	s = sum(v)
	c = s * ϵ / length(valid_inds)
	d = one(T)/s - ϵ #value to add to actions that are maximizing
	for i in valid_inds
		if v[i] == 1
			v[i] = d + c
		else
			v[i] = c
		end
	end
	return v
end

# ╔═╡ 0adcbce8-2be5-48ef-af43-04815e10dc5c
function make_greedy_policy!(v::AbstractVector{T}; c = 1000) where T<:Real
	(vmin, vmax) = extrema(v)
	if vmin == vmax
		v .= zero(T)
		v .= one(T) / length(v)
	else
		v .= (v .- vmax) ./ abs(vmin - vmax)
		v .= exp.(c .* v)
		v .= v ./ sum(v)
	end
	return v
end

# ╔═╡ 0899f37c-5def-4d15-8ca3-ebdec8e96b43
function begin_value_iteration_v(mdp::M, γ::T, V::Vector{T}; θ = eps(zero(T)), nmax=typemax(Int64)) where {T<:Real, M <: CompleteMDP{T}}
	valuelist = [copy(V)]
	value_iteration_v!(V, θ, mdp, γ, nmax, valuelist)

	π = form_random_policy(mdp)
	make_greedy_policy!(π, mdp, V, γ)
	return (valuelist, π)
end

# ╔═╡ 9a1b250f-b404-4db3-a4b7-4cd33b79d921
function create_greedy_policy(Q::Matrix{T}; c = 1000, π = copy(Q)) where T<:Real
	vhold = zeros(T, size(Q, 1))
	for j in 1:size(Q, 2)
		vhold .= Q[:, j]
		make_greedy_policy!(vhold; c = c)
		π[:, j] .= vhold
	end
	return π
end

# ╔═╡ 324181f0-b890-4198-9b4b-c36547e6629a
function create_ϵ_greedy_policy(Q::Matrix{T}, ϵ::T; π = copy(Q), get_valid_inds = j -> 1:size(Q, 1)) where T<:Real
	vhold = zeros(T, size(Q, 1))
	for j in 1:size(Q, 2)
		vhold .= Q[:, j]
		make_ϵ_greedy_policy!(vhold, ϵ; valid_inds = get_valid_inds(j))
		π[:, j] .= vhold
	end
	return π
end

# ╔═╡ 5afad18b-1d87-450e-a0ff-8c1249d663ed
function tabular_dynaQ(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, n = 100, qinit = zero(T), ϵinit = one(T)/10, Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), decay_ϵ = false, history_state::S = first(mdp.states), save_history = false, update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ)) where {S, A, F, G, H, T<:AbstractFloat}
	#each column contains the index of the state reached from the state represented by the column index while taking the action represented by the row index.  the values are initialized at 0 which represents a transition that has not occured
	state_transition_map = zeros(Int64, length(mdp.actions), length(mdp.states))

	#each column contains the reward received from the state represented by the column index while taking the action represented by the row index.  the state_transition_map can be used to determine if any of these values have been updated from the zero initialization
	reward_transition_map = zeros(T, length(mdp.actions), length(mdp.states))

	history = Dict{Int64, Set{Int64}}()

	π = copy(πinit)
	Q = copy(Qinit)
	ϵ = ϵinit
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q[:, terminds] .= zero(T)
	
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)

	function updateQ!(Q, i_s, i_a, i_s′, r)
		qmax = maximum(Q[i, i_s′] for i in eachindex(mdp.actions))
		Q[i_a, i_s] += α*(r + γ*qmax - Q[i_a, i_s])
	end

	function q_planning!(Q, history)
		for _ in 1:n
			i_s = rand(keys(history))
			i_a = rand(history[i_s])
			i_s′ = state_transition_map[i_a, i_s]
			r = reward_transition_map[i_a, i_s]
			updateQ!(Q, i_s, i_a, i_s′, r)
		end
	end
	
	for ep in 1:num_episodes
		s = mdp.state_init()
		l = 0
		rtot = zero(T)
		while !mdp.isterm(s)
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			updateQ!(Q, i_s, i_a, i_s′, r)
			state_transition_map[i_a, i_s] = i_s′
			reward_transition_map[i_a, i_s] = r
			#save state action pair visited
			if haskey(history, i_s)
				push!(history[i_s], i_a)
			else
				history[i_s] = Set([i_a])
			end
			q_planning!(Q, history)
			π = create_ϵ_greedy_policy(Q, ϵ; π = π)
			s = s′
			l += 1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end
	return Q, steps, rewards, state_transition_map, reward_transition_map
end

# ╔═╡ f3f05eb3-db68-44c8-806e-d09127276f4d
function figure8_4()
	dynaq_results = tabular_dynaQ(maze8_2.env, maze8_2.start, maze8_2.goal, maze8_2.states, maze8_2.actions, 0.1, 0.95, 5, 250; qinit=0.0, rinit = 0.0)

	# t1 = scatter(y = cumsum(results[1][4]), name = "no planning steps")
	# t2 = scatter(y = cumsum(results[2][4]), name = "5 planning steps")
	# t3 = scatter(y = cumsum(results[3][4]), name = "50 planning steps")
	# plot([t1, t2, t3], Layout(legend_orientation="h"))
	plot(cumsum(dynaq_results[4]), Layout(title="5 Planning Steps DynaQ vs DynaQ+ on Blocking Maze"))
end

# ╔═╡ 79069fa0-56bc-4dca-9ccd-873c370bf9f8
figure8_4()

# ╔═╡ 0f37ec0a-b737-478b-bf6a-027899250c4e
#take a step in the environment from state s using policy π and generate the subsequent action selection as well
function sarsa_step(mdp::MDP_TD{S, A, F, G, H}, π::Matrix{T}, s::S, a::A) where {S, A, F<:Function, G<:Function, H<:Function, T<:Real}
	(r, s′) = mdp.step(s, a)
	i_s′ = mdp.statelookup[s′]
	i_a′ = sample_action(π, i_s′)
	a′ = mdp.actions[i_a′]
	return (s′, i_s′, r, a′, i_a′)
end

# ╔═╡ f424edac-388d-4465-900f-9459d2a88f79
#take a step in the environment from state s using policy π and generate the subsequent action selection as well
function init_step(mdp::MDP_TD{S, A, F, G, H}, π::Matrix{T}, s::S) where {S, A, F<:Function, G<:Function, H<:Function, T<:Real}
	i_s = mdp.statelookup[s]
	i_a = sample_action(π, i_s)
	a = mdp.actions[i_a]
	return (i_s, i_a, a)
end

# ╔═╡ 4494cb61-ee2c-467b-9bf6-0afb59023e91
function sarsa(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), history_state::S = first(mdp.states), update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), save_history = false, decay_ϵ = false) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q = copy(Qinit)
	Q[:, terminds] .= zero(T)
	π = copy(πinit)
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)

	if save_history
		action_history = Vector{A}(undef, num_episodes)
	end

	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		(i_s, i_a, a) = init_step(mdp, π, s)
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			(s′, i_s′, r, a′, i_a′) = sarsa_step(mdp, π, s, a)
			if save_history && (s == history_state)
				action_history[ep] = a
			end
			Q[i_a, i_s] += α * (r + γ*Q[i_a′, i_s′] - Q[i_a, i_s])
			
			#update terms for next step
			vhold .= Q[:, i_s]
			update_policy!(vhold, ϵ, s)
			π[:, i_s] .= vhold
			s = s′
			a = a′
			i_s = i_s′
			i_a = i_a′
			
			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	default_return =  Q, π, steps, rewards
	save_history && return (default_return..., action_history)
	return default_return
end

# ╔═╡ 143fff7d-0bb2-43b4-b810-53784fe848bd
function q_learning(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), decay_ϵ = false, history_state::S = first(mdp.states), save_history = false, update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ)) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q = copy(Qinit)
	Q[:, terminds] .= zero(T)
	π = copy(πinit)
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)
	
	if save_history
		history_actions = Vector{A}(undef, num_episodes)
	end
	
	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			if save_history && (s == history_state)
				history_actions[ep] = a
			end
			qmax = maximum(Q[i, i_s′] for i in eachindex(mdp.actions))
			Q[i_a, i_s] += α*(r + γ*qmax - Q[i_a, i_s])
			
			#update terms for next step
			vhold .= Q[:, i_s]
			update_policy!(vhold, ϵ, s)
			π[:, i_s] .= vhold
			s = s′
			
			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	save_history && return Q, π, steps, rewards, history_actions
	return Q, π, steps, rewards
end

# ╔═╡ 9be963b9-f3a1-4f92-8ff9-f5be75ed52f2
function expected_sarsa(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), decay_ϵ = false, save_history = false, save_state = first(mdp.states)) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q = copy(Qinit)
	Q[:, terminds] .= zero(T)
	π = copy(πinit)
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)
	if save_history
		action_history = Vector{A}(undef, num_episodes)
	end
	
	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			if save_history && (s == save_state)
				action_history[ep] = a
			end
			q_expected = sum(π[i, i_s′]*Q[i, i_s′] for i in eachindex(mdp.actions))
			Q[i_a, i_s] += α*(r + γ*q_expected - Q[i_a, i_s])
			
			#update terms for next step
			vhold .= Q[:, i_s]
			update_policy!(vhold, ϵ, s)
			π[:, i_s] .= vhold
			s = s′
			
			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	base_return = (Q, π, steps, rewards)
	save_history && return (base_return..., action_history)
	return base_return
end

# ╔═╡ 113d2bc2-1f77-479f-86e5-a65b20672d7a
function double_expected_sarsa(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit::Matrix{T} = initialize_state_action_value(mdp; qinit=qinit), decay_ϵ = false, target_policy_function! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), behavior_policy_function! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), πinit_target::Matrix{T} = create_ϵ_greedy_policy(Qinit, ϵinit), πinit_behavior::Matrix{T} = create_ϵ_greedy_policy(Qinit, ϵinit), save_state::S = first(mdp.states), save_history = false) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	
	Q1 = copy(Qinit)
	Q2 = copy(Qinit) 
	Q1[:, terminds] .= zero(T)
	Q2[:, terminds] .= zero(T)
	π_target1 = copy(πinit_target)
	π_target2 = copy(πinit_target)
	π_behavior = copy(πinit_behavior)
	vhold1 = zeros(T, length(mdp.actions))
	vhold2 = zeros(T, length(mdp.actions))
	vhold3 = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)

	if save_history
		action_history = Vector{A}(undef, num_episodes)
	end
	
	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π_behavior, s)
			if save_history && (s == save_state)
				action_history[ep] = a
			end
			
			# q_expected = sum(π_target[i, i_s′]*(Q1[i, i_s′]*toggle + Q2[i, i_s′]*(1-toggle)) for i in eachindex(mdp.actions))
			toggle = rand() < 0.5
			q_expected = if toggle 
				sum(π_target2[i, i_s′]*Q1[i, i_s′] for i in eachindex(mdp.actions))
			else
				sum(π_target1[i, i_s′]*Q2[i, i_s′] for i in eachindex(mdp.actions))
			end

			if toggle
				Q2[i_a, i_s] += α*(r + γ*q_expected - Q2[i_a, i_s])
			else
				Q1[i_a, i_s] += α*(r + γ*q_expected - Q1[i_a, i_s])
			end
			
			#update terms for next step
			if toggle
				vhold2 .= Q2[:, i_s]
				target_policy_function!(vhold2, ϵ, s)
				π_target2[:, i_s] .= vhold2
			else
				vhold1 .= Q1[:, i_s]
				target_policy_function!(vhold1, ϵ, s)
				π_target1[:, i_s] .= vhold1
			end
			
			vhold3 .= vhold1 .+ vhold2
			behavior_policy_function!(vhold3, ϵ, s)
			π_behavior[:, i_s] .= vhold3
			
			s = s′

			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	Q1 .+= Q2
	Q1 ./= 2
	plain_return = Q1, create_greedy_policy(Q1), steps, rewards

	save_history && return (plain_return..., action_history)
	return plain_return
end

# ╔═╡ 5d2abde0-7128-41c3-bd1f-b6940492d1ae
function double_q_learning(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; 
	num_episodes = 1000, 
	qinit = zero(T), 
	ϵinit = one(T)/10, 
	Qinit::Matrix{T} = initialize_state_action_value(mdp; qinit=qinit), 
	decay_ϵ = false, 
	target_policy_function! = (v, ϵ, s) -> make_greedy_policy!(v), 
	behavior_policy_function! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), 
	πinit_target::Matrix{T} = create_greedy_policy(Qinit), 
	πinit_behavior::Matrix{T} = create_ϵ_greedy_policy(Qinit, ϵinit), 
	save_state::S = first(mdp.states), 
	save_history = false) where {S, A, F, G, H, T<:AbstractFloat} 
	
	double_expected_sarsa(mdp, α, γ; num_episodes = num_episodes, qinit = qinit, ϵinit = ϵinit, Qinit = Qinit, decay_ϵ = decay_ϵ, target_policy_function! = target_policy_function!, behavior_policy_function! = behavior_policy_function!, πinit_target = πinit_target, πinit_behavior = πinit_behavior, save_state = save_state, save_history = save_history)
end

# ╔═╡ 4e1c115a-4020-4a5a-a79a-56056892a953
md"""
## Gridworld Environment
"""

# ╔═╡ 729197ce-2c27-467d-ba5f-47a1ecd539f2
begin
	abstract type GridworldAction end
	struct Up <: GridworldAction end
	struct Down <: GridworldAction end
	struct Left <: GridworldAction end
	struct Right <: GridworldAction end
	struct UpRight <: GridworldAction end
	struct DownRight <: GridworldAction end
	struct UpLeft <: GridworldAction end
	struct DownLeft <: GridworldAction end
	struct Stay <: GridworldAction end
	
	struct GridworldState
		x::Int64
		y::Int64
	end

	rook_actions = [Up(), Down(), Left(), Right()]
	
	move(::Up, x, y) = (x, y+1)
	move(::Down, x, y) = (x, y-1)
	move(::Left, x, y) = (x-1, y)
	move(::Right, x, y) = (x+1, y)
	move(::UpRight, x, y) = (x+1, y+1)
	move(::UpLeft, x, y) = (x-1, y+1)
	move(::DownRight, x, y) = (x+1, y-1)
	move(::DownLeft, x, y) = (x-1, y-1)
	move(::Stay, x, y) = (x, y)
	apply_wind(w, x, y) = (x, y+w)
	const wind_vals = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]
end

# ╔═╡ 773ac9c5-c126-4e7d-b280-299adffcd840
function create_dynamic_maze(xmax, ymax, start::GridPoint, goal::GridPoint; dynamicobstacles=Dict(typemax(Int64) => Set{GridPoint}()))
	states = [GridPoint(x, y) for x in 1:xmax for y in 1:ymax if !in((x, y), obstacles)]
	actions = [Up(), Down(), Left(), Right()]
	sterm = goal

	function badpoint(s, obstacles)
		s.x < 1 && return true
		s.y < 1 && return true
		s.x > xmax && return true
		s.y > ymax && return true
		in(s, obstacles) && return true
		return false
	end

	function get_obstacles(τ)
		k = findfirst(k -> k <= τ, keys(dynamicobstacles))
		dynamicobstacles[k]
	end
		
	function env(s, a, τ)
		obstacles = get_obstacles(τ)
		s′ = move(s, a)
		badpoint(s′, obstacles) && return (s, 0.0)
		(s′ == goal) && return (goal, 1.0)
		return (s′, 0.0)
	end
	(states = states, actions = actions, start = start, goal = goal, env = env)
end

# ╔═╡ bb439641-30bd-495d-ba70-06b2e27efdbd
function make_gridworld(;actions = rook_actions, sterm = GridworldState(8, 4), start = GridworldState(1, 4), xmax = 10, ymax = 7, stepreward = 0.0f0, termreward = 1.0f0, iscliff = s -> false, iswall = s -> false, cliffreward = -100f0, goal2 = GridworldState(start.x, ymax), goal2reward = 0.0f0, usegoal2 = false)
	
	states = [GridworldState(x, y) for x in 1:xmax for y in 1:ymax]
	
	boundstate(x::Int64, y::Int64) = (clamp(x, 1, xmax), clamp(y, 1, ymax))
	
	function step(s::GridworldState, a::GridworldAction)
		(x, y) = move(a, s.x, s.y)
		s′ = GridworldState(boundstate(x, y)...)
		iswall(s′) && return s
		return s′
	end
		
	function isterm(s::GridworldState) 
		s == sterm && return true
		usegoal2 && (s == goal2) && return true
		return false
	end

	function tr(s::GridworldState, a::GridworldAction) 
		isterm(s) && return (0f0, s)
		s′ = step(s, a)
		iscliff(s′) && return (cliffreward, start)
		x = Float32(isterm(s′))
		usegoal2 && (s′ == goal2) && return (goal2reward, goal2)
		r = (1f0 - x)*stepreward + x*termreward
		(r, s′)
	end	
	MDP_TD(states, actions, () -> start, tr, isterm)
end	

# ╔═╡ c04c803c-cdca-4b8b-9c9d-e456ee677906
begin
	maze_walls = Set(GridworldState(x, y) for (x, y) in [(3, 3), (3, 4), (3, 5), (6, 2), (8, 4), (8, 5), (8, 6)])
	const dyna_maze = make_gridworld(;xmax = 9, ymax = 6, sterm = GridworldState(9, 6), iswall = s -> in(s, maze_walls))
end

# ╔═╡ cd139745-1877-43a2-97a0-3333e544cbd8
function figure8_2(;num_episodes = 50, α = 0.1f0, nlist = [0, 5, 50], γ = 0.95f0, samples = 5)
	traces = [begin
		step_avg = zeros(Int64, num_episodes)
		for s in 1:samples
			Random.seed!(s)
			(Q, steps, rewards, _, _) = tabular_dynaQ(dyna_maze, α, γ; num_episodes = num_episodes, n = n)
			step_avg .+= steps
		end
		scatter(x = 2:num_episodes, y = step_avg[2:end] ./ samples, name = "$n planning steps")
	end
	for n in nlist]
	plot(traces, Layout(xaxis_title = "Episodes", yaxis_title = "Steps per episode"))
end

# ╔═╡ 8dbc76fd-ac73-47ca-983e-0e90023390e3
figure8_2()

# ╔═╡ 77fde69f-2119-41eb-8993-a93b2c47ca7e
md"""
## Gridworld Visualization
"""

# ╔═╡ eded8b72-70f4-4579-ba69-2eca409fa684
function plot_path(mdp, states::Vector, sterm; title = "Optimal policy <br> path example", iscliff = s -> false, iswall = s -> false)
	xmax = maximum([s.x for s in mdp.states])
	ymax = maximum([s.y for s in mdp.states])
	start = mdp.state_init()
	goal = mdp.states[findlast(mdp.isterm(s) for s in mdp.states)]
	start_trace = scatter(x = [start.x + 0.5], y = [start.y + 0.5], mode = "text", text = ["S"], textposition = "left", showlegend=false)
	finish_trace = scatter(x = [goal.x + .5], y = [goal.y + .5], mode = "text", text = ["G"], textposition = "left", showlegend=false)
	
	path_traces = [scatter(x = [states[i].x + 0.5, states[i+1].x + 0.5], y = [states[i].y + 0.5, states[i+1].y + 0.5], line_color = "blue", mode = "lines", showlegend=false, name = "Optimal Path") for i in 1:length(states)-1]
	finalpath = scatter(x = [states[end].x + 0.5, sterm.x + .5], y = [states[end].y + 0.5, sterm.y + 0.5], line_color = "blue", mode = "lines", showlegend=false, name = "Optimal Path")

	h1 = 30*ymax
	traces = [start_trace; finish_trace; path_traces; finalpath]

	cliff_squares = filter(iscliff, mdp.states)
	for s in cliff_squares
		push!(traces, scatter(x = [s.x + 0.6], y = [s.y+0.5], mode = "text", text = ["C"], textposition = "left", showlegend = false))
	end


	wall_squares = filter(iswall, mdp.states)
	for s in wall_squares
		push!(traces, scatter(x = [s.x + 0.8], y = [s.y+0.5], mode = "text", text = ["W"], textposition = "left", showlegend = false))
	end

	plot(traces, Layout(xaxis = attr(showgrid = true, showline = true, gridwith = 1, gridcolor = "black", zeroline = true, linecolor = "black", mirror=true, tickvals = 1:xmax, ticktext = fill("", 10), range = [1, xmax+1]), yaxis = attr(linecolor="black", mirror = true, gridcolor = "black", showgrid = true, gridwidth = 1, showline = true, tickvals = 1:ymax, ticktext = fill("", ymax), range = [1, ymax+1]), width = max(30*xmax, 200), height = max(h1, 200), autosize = false, padding=0, paper_bgcolor = "rgba(0, 0, 0, 0)", title = attr(text = title, font_size = 14, x = 0.5)))
end

# ╔═╡ 1f7d77a6-d774-436d-a745-5a160cc15f2b
function plot_path(mdp, π::Matrix; max_steps = 100, kwargs...)
	(states, actions, rewards, sterm) = runepisode(mdp, π; max_steps = max_steps)
	plot_path(mdp, states, sterm; kwargs...)
end

# ╔═╡ 502a7125-4460-4d39-be14-4852fb6d9ad2
plot_path(mdp; title = "Random policy <br> path example", kwargs...) = plot_path(mdp, make_random_policy(mdp); title = title, kwargs...)

# ╔═╡ 4b3604db-0c1b-4770-95b1-5f5bb34d071b
function addelements(e1, e2)
	@htl("""
	$e1
	$e2
	""")
end

# ╔═╡ 39c96fc8-8259-46e3-88a0-a14eb6752b5c
function show_grid_value(mdp, Q, name; scale = 1.0, title = "", sigdigits = 2)
	width = maximum(s.x for s in mdp.states)
	height = maximum(s.y for s in mdp.states)
	start = mdp.state_init()
	termind = findfirst(mdp.isterm, mdp.states)
	sterm = mdp.states[termind]
	ngrid = width*height

	displayvalue(Q::Matrix, i) = round(maximum(Q[:, i]), sigdigits = sigdigits)
	displayvalue(V::Vector, i) = round(V[i], sigdigits = sigdigits)
	@htl("""
		<div style = "display: flex; transform: scale($scale); background-color: white; color: black; font-size: 16px;">
			<div>
				$title
				<div class = "gridworld $name value">
					$(HTML(mapreduce(i -> """<div class = "gridcell $name value" x = "$(mdp.states[i].x)" y = "$(mdp.states[i].y)" style = "grid-row: $(height - mdp.states[i].y + 1); grid-column: $(mdp.states[i].x); font-size: 12px; color: black; $(displayvalue(Q, i) != 0 ? "background-color: lightblue;" : "")">$(displayvalue(Q, i))</div>""", *, eachindex(mdp.states))))
				</div>
			</div>
		</div>
	
		<style>
			.$name.value.gridworld {
				display: grid;
				grid-template-columns: repeat($width, 20px);
				grid-template-rows: repeat($height, 20px);
				background-color: white;
			}

			.$name.value[x="$(start.x)"][y="$(start.y)"] {
				content: '';
				background-color: rgba(0, 255, 0, 0.5);
				
			}

			.$name.value[x="$(sterm.x)"][y="$(sterm.y)"] {
				content: '';
				background-color: rgba(255, 0, 0, 0.5);
				
			}

		</style>
	""")
end

# ╔═╡ 3bf9e526-826d-42b8-84ee-75f1c7f79c69
function display_rook_policy(v::Vector{T}; scale = 1.0) where T<:AbstractFloat
	@htl("""
		<div style = "display: flex; align-items: center; justify-content: center; transform: scale($scale);">
		<div class = "downarrow" style = "position: absolute; transform: rotate(180deg); opacity: $(v[1]);"></div>	
		<div class = "downarrow" style = "position: absolute; opacity: $(v[2])"></div>
		<div class = "downarrow" style = "position: absolute; transform: rotate(90deg); opacity: $(v[3])"></div>
		<div class = "downarrow" style = "transform: rotate(-90deg); opacity: $(v[4])"></div>
		</div>
	""")
end

# ╔═╡ 9d69687a-8df6-4e74-aa99-fbfcc84bcccf
const rook_action_display = @htl("""
<div style = "display: flex; flex-direction: column; align-items: center; justify-content: center; color: black; background-color: rgba(100, 100, 100, 0.1);">
	<div style = "display: flex; align-items: center; justify-content: center;">
	<div class = "downarrow" style = "transform: rotate(90deg);"></div>
	<div class = "downarrow" style = "position: absolute; transform: rotate(180deg);"></div>
	<div class = "downarrow" style = "position: absolute; transform: rotate(270deg);"></div>
	<div class = "downarrow" style = "position: absolute;"></div>
	</div>
	<div>Actions</div>
</div>
""")

# ╔═╡ 563b6dbd-ce51-4904-b1cc-d766bd1fd1d6
@htl("""
<div style = "background-color: white; color: black; display: flex; align-items: center; justify-content: center;">
<div>$(plot_path(dyna_maze; title = "Random policy path example in Dyna Maze", max_steps = 10000, iswall = s -> in(s, maze_walls)))</div>
<div>$rook_action_display</div>
</div>
""")

# ╔═╡ f5e52b2f-ea14-423d-8ca9-2ed68cd27c69
function show_grid_policy(mdp, π, name; display_function = display_rook_policy, action_display = rook_action_display, scale = 1.0)
	width = maximum(s.x for s in mdp.states)
	height = maximum(s.y for s in mdp.states)
	start = mdp.state_init()
	termind = findfirst(mdp.isterm, mdp.states)
	sterm = mdp.states[termind]
	ngrid = width*height
	@htl("""
		<div style = "display: flex; transform: scale($scale); background-color: white;">
			<div>
				<div class = "gridworld $name">
					$(HTML(mapreduce(i -> """<div class = "gridcell $name" x = "$(mdp.states[i].x)" y = "$(mdp.states[i].y)" style = "grid-row: $(height - mdp.states[i].y + 1); grid-column: $(mdp.states[i].x);">$(display_function(π[:, i], scale =0.8))</div>""", *, eachindex(mdp.states))))
				</div>
			</div>
			<div style = "display: flex; flex-direction: column; align-items: flex-start; justify-content: flex-end; color: black; font-size: 18px; width: 5em; margin-left: 1em;">
				$(action_display)
			</div>
		</div>
	
		<style>
			.$name.gridworld {
				display: grid;
				grid-template-columns: repeat($width, 40px);
				grid-template-rows: repeat($height, 40px);
				background-color: white;

			.$name[x="$(start.x)"][y="$(start.y)"]::before {
				content: 'S';
				position: absolute;
				color: green;
				opacity: 1.0;
			}

			.$name[x="$(sterm.x)"][y="$(sterm.y)"]::before {
				content: 'G';
				position: absolute;
				color: red;
				opacity: 1.0;
			}

		</style>
	""")
end

# ╔═╡ 0d0bbf62-b1ac-45f6-8a92-1e77b0709cb3
function figure8_3(num_episodes; α = 0.1f0, nlist = [0, 50], γ = 0.95f0,)
	(Q1, _) = tabular_dynaQ(dyna_maze, α, γ; num_episodes = num_episodes, n = first(nlist))
	d1 = show_grid_policy(dyna_maze, create_greedy_policy(Q1), "test")
	(Q2, _) = tabular_dynaQ(dyna_maze, α, γ; num_episodes = num_episodes, n = last(nlist))
	d2 = show_grid_policy(dyna_maze, create_greedy_policy(Q2), "test")
	@htl("""
	<div style = "display: flex;">
	<div>Without planning (n = $(first(nlist)))$d1</div>
	<div>With planning (n = $(last(nlist)))$d2</div>
	</div>
	""")
end

# ╔═╡ 4d4baa61-b5bd-4bcf-a491-9a35a1695f0b
figure8_3(num_episodes_8_3)

# ╔═╡ 0f9080af-f166-4a78-a003-8df07f6c27d4
HTML("""
<style>
	.downarrow {
		display: flex;
		justify-content: center;
		align-items: center;
		flex-direction: column;
	}

	.downarrow::before {
		content: '';
		width: 2px;
		height: 40px;
		background-color: black;
	}
	.downarrow::after {
		content: '';
		width: 0px;
		height: 0px;
		border-left: 5px solid transparent;
		border-right: 5px solid transparent;
		border-top: 10px solid black;
	}

	.gridcell {
			display: flex;
			justify-content: center;
			align-items: center;
			border: 1px solid black;
		}

	.windbox {
		height: 40px;
		width: 40px;
		display: flex;
		justify-content: center;
		align-items: center;
		transform: rotate(180deg);
		background-color: green;
	}

	.windbox * {
		background-color: green;
		color: green;
	}

	.windbox[w="0"] {
		opacity: 0.0; 
	}

	.windbox[w="1"] {
		opacity: 0.5;
	}

	.windbox[w="2"] {
		opacity: 1.0;
	}
</style>
""")

# ╔═╡ 00000000-0000-0000-0000-000000000001
PLUTO_PROJECT_TOML_CONTENTS = """
[deps]
DataStructures = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
HypertextLiteral = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
PlutoPlotly = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
PlutoUI = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
Random = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"
StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"
Statistics = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
StatsBase = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
Transducers = "28d57a85-8fef-5791-bfe6-a80928e7c999"

[compat]
DataStructures = "~0.18.18"
HypertextLiteral = "~0.9.5"
PlutoPlotly = "~0.4.6"
PlutoUI = "~0.7.58"
StaticArrays = "~1.9.3"
StatsBase = "~0.34.2"
Transducers = "~0.4.81"
"""

# ╔═╡ 00000000-0000-0000-0000-000000000002
PLUTO_MANIFEST_TOML_CONTENTS = """
# This file is machine-generated - editing it directly is not advised

julia_version = "1.10.2"
manifest_format = "2.0"
project_hash = "212caf4693e4c20344a7076d4e5ddd84d5c9344a"

[[deps.AbstractPlutoDingetjes]]
deps = ["Pkg"]
git-tree-sha1 = "0f748c81756f2e5e6854298f11ad8b2dfae6911a"
uuid = "6e696c72-6542-2067-7265-42206c756150"
version = "1.3.0"

[[deps.Accessors]]
deps = ["CompositionsBase", "ConstructionBase", "Dates", "InverseFunctions", "LinearAlgebra", "MacroTools", "Markdown", "Test"]
git-tree-sha1 = "c0d491ef0b135fd7d63cbc6404286bc633329425"
uuid = "7d9f7c33-5ae7-4f3b-8dc6-eff91059b697"
version = "0.1.36"

    [deps.Accessors.extensions]
    AccessorsAxisKeysExt = "AxisKeys"
    AccessorsIntervalSetsExt = "IntervalSets"
    AccessorsStaticArraysExt = "StaticArrays"
    AccessorsStructArraysExt = "StructArrays"
    AccessorsUnitfulExt = "Unitful"

    [deps.Accessors.weakdeps]
    AxisKeys = "94b1ba4f-4ee9-5380-92f1-94cde586c3c5"
    IntervalSets = "8197267c-284f-5f27-9208-e0e47529a953"
    Requires = "ae029012-a4dd-5104-9daa-d747884805df"
    StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"
    StructArrays = "09ab397b-f2b6-538f-b94a-2f83cf4a842a"
    Unitful = "1986cc42-f94f-5a68-af5c-568840ba703d"

[[deps.Adapt]]
deps = ["LinearAlgebra", "Requires"]
git-tree-sha1 = "6a55b747d1812e699320963ffde36f1ebdda4099"
uuid = "79e6a3ab-5dfb-504d-930d-738a2a938a0e"
version = "4.0.4"
weakdeps = ["StaticArrays"]

    [deps.Adapt.extensions]
    AdaptStaticArraysExt = "StaticArrays"

[[deps.ArgCheck]]
git-tree-sha1 = "a3a402a35a2f7e0b87828ccabbd5ebfbebe356b4"
uuid = "dce04be8-c92d-5529-be00-80e4d2c0e197"
version = "2.3.0"

[[deps.ArgTools]]
uuid = "0dad84c5-d112-42e6-8d28-ef12dabb789f"
version = "1.1.1"

[[deps.Artifacts]]
uuid = "56f22d72-fd6d-98f1-02f0-08ddc0907c33"

[[deps.BangBang]]
deps = ["Accessors", "Compat", "ConstructionBase", "InitialValues", "LinearAlgebra", "Requires"]
git-tree-sha1 = "490e739172eb18f762e68dc3b928cad2a077983a"
uuid = "198e06fe-97b7-11e9-32a5-e1d131e6ad66"
version = "0.4.1"

    [deps.BangBang.extensions]
    BangBangChainRulesCoreExt = "ChainRulesCore"
    BangBangDataFramesExt = "DataFrames"
    BangBangStaticArraysExt = "StaticArrays"
    BangBangStructArraysExt = "StructArrays"
    BangBangTablesExt = "Tables"
    BangBangTypedTablesExt = "TypedTables"

    [deps.BangBang.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    DataFrames = "a93c6f00-e57d-5684-b7b6-d8193f3e46c0"
    StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"
    StructArrays = "09ab397b-f2b6-538f-b94a-2f83cf4a842a"
    Tables = "bd369af6-aec1-5ad0-b16a-f7cc5008161c"
    TypedTables = "9d95f2ec-7b3d-5a63-8d20-e2491e220bb9"

[[deps.Base64]]
uuid = "2a0f44e3-6c83-55bd-87e4-b1978d98bd5f"

[[deps.BaseDirs]]
git-tree-sha1 = "3e93fcd95fe8db4704e98dbda14453a0bfc6f6c3"
uuid = "18cc8868-cbac-4acf-b575-c8ff214dc66f"
version = "1.2.3"

[[deps.Baselet]]
git-tree-sha1 = "aebf55e6d7795e02ca500a689d326ac979aaf89e"
uuid = "9718e550-a3fa-408a-8086-8db961cd8217"
version = "0.1.1"

[[deps.ColorSchemes]]
deps = ["ColorTypes", "ColorVectorSpace", "Colors", "FixedPointNumbers", "PrecompileTools", "Random"]
git-tree-sha1 = "67c1f244b991cad9b0aa4b7540fb758c2488b129"
uuid = "35d6a980-a343-548e-a6ea-1d62b119f2f4"
version = "3.24.0"

[[deps.ColorTypes]]
deps = ["FixedPointNumbers", "Random"]
git-tree-sha1 = "eb7f0f8307f71fac7c606984ea5fb2817275d6e4"
uuid = "3da002f7-5984-5a60-b8a6-cbb66c0b333f"
version = "0.11.4"

[[deps.ColorVectorSpace]]
deps = ["ColorTypes", "FixedPointNumbers", "LinearAlgebra", "Requires", "Statistics", "TensorCore"]
git-tree-sha1 = "a1f44953f2382ebb937d60dafbe2deea4bd23249"
uuid = "c3611d14-8923-5661-9e6a-0046d554d3a4"
version = "0.10.0"

    [deps.ColorVectorSpace.extensions]
    SpecialFunctionsExt = "SpecialFunctions"

    [deps.ColorVectorSpace.weakdeps]
    SpecialFunctions = "276daf66-3868-5448-9aa4-cd146d93841b"

[[deps.Colors]]
deps = ["ColorTypes", "FixedPointNumbers", "Reexport"]
git-tree-sha1 = "fc08e5930ee9a4e03f84bfb5211cb54e7769758a"
uuid = "5ae59095-9a9b-59fe-a467-6f913c188581"
version = "0.12.10"

[[deps.Compat]]
deps = ["TOML", "UUIDs"]
git-tree-sha1 = "c955881e3c981181362ae4088b35995446298b80"
uuid = "34da2185-b29b-5c13-b0c7-acf172513d20"
version = "4.14.0"
weakdeps = ["Dates", "LinearAlgebra"]

    [deps.Compat.extensions]
    CompatLinearAlgebraExt = "LinearAlgebra"

[[deps.CompilerSupportLibraries_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "e66e0078-7015-5450-92f7-15fbd957f2ae"
version = "1.1.0+0"

[[deps.CompositionsBase]]
git-tree-sha1 = "802bb88cd69dfd1509f6670416bd4434015693ad"
uuid = "a33af91c-f02d-484b-be07-31d278c5ca2b"
version = "0.1.2"
weakdeps = ["InverseFunctions"]

    [deps.CompositionsBase.extensions]
    CompositionsBaseInverseFunctionsExt = "InverseFunctions"

[[deps.ConstructionBase]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "260fd2400ed2dab602a7c15cf10c1933c59930a2"
uuid = "187b0558-2788-49d3-abe0-74a17ed4e7c9"
version = "1.5.5"

    [deps.ConstructionBase.extensions]
    ConstructionBaseIntervalSetsExt = "IntervalSets"
    ConstructionBaseStaticArraysExt = "StaticArrays"

    [deps.ConstructionBase.weakdeps]
    IntervalSets = "8197267c-284f-5f27-9208-e0e47529a953"
    StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"

[[deps.DataAPI]]
git-tree-sha1 = "abe83f3a2f1b857aac70ef8b269080af17764bbe"
uuid = "9a962f9c-6df0-11e9-0e5d-c546b8b5ee8a"
version = "1.16.0"

[[deps.DataStructures]]
deps = ["Compat", "InteractiveUtils", "OrderedCollections"]
git-tree-sha1 = "0f4b5d62a88d8f59003e43c25a8a90de9eb76317"
uuid = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
version = "0.18.18"

[[deps.DataValueInterfaces]]
git-tree-sha1 = "bfc1187b79289637fa0ef6d4436ebdfe6905cbd6"
uuid = "e2d170a0-9d28-54be-80f0-106bbe20a464"
version = "1.0.0"

[[deps.Dates]]
deps = ["Printf"]
uuid = "ade2ca70-3891-5945-98fb-dc099432e06a"

[[deps.DefineSingletons]]
git-tree-sha1 = "0fba8b706d0178b4dc7fd44a96a92382c9065c2c"
uuid = "244e2a9f-e319-4986-a169-4d1fe445cd52"
version = "0.1.2"

[[deps.DelimitedFiles]]
deps = ["Mmap"]
git-tree-sha1 = "9e2f36d3c96a820c678f2f1f1782582fcf685bae"
uuid = "8bb1440f-4735-579b-a4ab-409b98df4dab"
version = "1.9.1"

[[deps.Distributed]]
deps = ["Random", "Serialization", "Sockets"]
uuid = "8ba89e20-285c-5b6f-9357-94700520ee1b"

[[deps.DocStringExtensions]]
deps = ["LibGit2"]
git-tree-sha1 = "2fb1e02f2b635d0845df5d7c167fec4dd739b00d"
uuid = "ffbed154-4ef7-542d-bbb7-c09d3a79fcae"
version = "0.9.3"

[[deps.Downloads]]
deps = ["ArgTools", "FileWatching", "LibCURL", "NetworkOptions"]
uuid = "f43a241f-c20a-4ad4-852c-f6b1247861c6"
version = "1.6.0"

[[deps.FileWatching]]
uuid = "7b1f6079-737a-58dc-b8bc-7a2ca5c1b5ee"

[[deps.FixedPointNumbers]]
deps = ["Statistics"]
git-tree-sha1 = "335bfdceacc84c5cdf16aadc768aa5ddfc5383cc"
uuid = "53c48c17-4a7d-5ca2-90c5-79b7896eea93"
version = "0.8.4"

[[deps.Future]]
deps = ["Random"]
uuid = "9fa8497b-333b-5362-9e8d-4d0656e87820"

[[deps.Hyperscript]]
deps = ["Test"]
git-tree-sha1 = "179267cfa5e712760cd43dcae385d7ea90cc25a4"
uuid = "47d2ed2b-36de-50cf-bf87-49c2cf4b8b91"
version = "0.0.5"

[[deps.HypertextLiteral]]
deps = ["Tricks"]
git-tree-sha1 = "7134810b1afce04bbc1045ca1985fbe81ce17653"
uuid = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
version = "0.9.5"

[[deps.IOCapture]]
deps = ["Logging", "Random"]
git-tree-sha1 = "8b72179abc660bfab5e28472e019392b97d0985c"
uuid = "b5f81e59-6552-4d32-b1f0-c071b021bf89"
version = "0.2.4"

[[deps.InitialValues]]
git-tree-sha1 = "4da0f88e9a39111c2fa3add390ab15f3a44f3ca3"
uuid = "22cec73e-a1b8-11e9-2c92-598750a2cf9c"
version = "0.3.1"

[[deps.InteractiveUtils]]
deps = ["Markdown"]
uuid = "b77e0a4c-d291-57a0-90e8-8db25a27a240"

[[deps.InverseFunctions]]
deps = ["Test"]
git-tree-sha1 = "896385798a8d49a255c398bd49162062e4a4c435"
uuid = "3587e190-3f89-42d0-90ee-14403ec27112"
version = "0.1.13"
weakdeps = ["Dates"]

    [deps.InverseFunctions.extensions]
    DatesExt = "Dates"

[[deps.IrrationalConstants]]
git-tree-sha1 = "630b497eafcc20001bba38a4651b327dcfc491d2"
uuid = "92d709cd-6900-40b7-9082-c6be49f344b6"
version = "0.2.2"

[[deps.IteratorInterfaceExtensions]]
git-tree-sha1 = "a3f24677c21f5bbe9d2a714f95dcd58337fb2856"
uuid = "82899510-4779-5014-852e-03e436cf321d"
version = "1.0.0"

[[deps.JSON]]
deps = ["Dates", "Mmap", "Parsers", "Unicode"]
git-tree-sha1 = "31e996f0a15c7b280ba9f76636b3ff9e2ae58c9a"
uuid = "682c06a0-de6a-54ab-a142-c8b1cf79cde6"
version = "0.21.4"

[[deps.LaTeXStrings]]
git-tree-sha1 = "50901ebc375ed41dbf8058da26f9de442febbbec"
uuid = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
version = "1.3.1"

[[deps.LibCURL]]
deps = ["LibCURL_jll", "MozillaCACerts_jll"]
uuid = "b27032c2-a3e7-50c8-80cd-2d36dbcbfd21"
version = "0.6.4"

[[deps.LibCURL_jll]]
deps = ["Artifacts", "LibSSH2_jll", "Libdl", "MbedTLS_jll", "Zlib_jll", "nghttp2_jll"]
uuid = "deac9b47-8bc7-5906-a0fe-35ac56dc84c0"
version = "8.4.0+0"

[[deps.LibGit2]]
deps = ["Base64", "LibGit2_jll", "NetworkOptions", "Printf", "SHA"]
uuid = "76f85450-5226-5b5a-8eaa-529ad045b433"

[[deps.LibGit2_jll]]
deps = ["Artifacts", "LibSSH2_jll", "Libdl", "MbedTLS_jll"]
uuid = "e37daf67-58a4-590a-8e99-b0245dd2ffc5"
version = "1.6.4+0"

[[deps.LibSSH2_jll]]
deps = ["Artifacts", "Libdl", "MbedTLS_jll"]
uuid = "29816b5a-b9ab-546f-933c-edad1886dfa8"
version = "1.11.0+1"

[[deps.Libdl]]
uuid = "8f399da3-3557-5675-b5ff-fb832c97cbdb"

[[deps.LinearAlgebra]]
deps = ["Libdl", "OpenBLAS_jll", "libblastrampoline_jll"]
uuid = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"

[[deps.LogExpFunctions]]
deps = ["DocStringExtensions", "IrrationalConstants", "LinearAlgebra"]
git-tree-sha1 = "18144f3e9cbe9b15b070288eef858f71b291ce37"
uuid = "2ab3a3ac-af41-5b50-aa03-7779005ae688"
version = "0.3.27"

    [deps.LogExpFunctions.extensions]
    LogExpFunctionsChainRulesCoreExt = "ChainRulesCore"
    LogExpFunctionsChangesOfVariablesExt = "ChangesOfVariables"
    LogExpFunctionsInverseFunctionsExt = "InverseFunctions"

    [deps.LogExpFunctions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    ChangesOfVariables = "9e997f8a-9a97-42d5-a9f1-ce6bfc15e2c0"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.Logging]]
uuid = "56ddb016-857b-54e1-b83d-db4d58db5568"

[[deps.MIMEs]]
git-tree-sha1 = "65f28ad4b594aebe22157d6fac869786a255b7eb"
uuid = "6c6e2e6c-3030-632d-7369-2d6c69616d65"
version = "0.1.4"

[[deps.MacroTools]]
deps = ["Markdown", "Random"]
git-tree-sha1 = "2fa9ee3e63fd3a4f7a9a4f4744a52f4856de82df"
uuid = "1914dd2f-81c6-5fcd-8719-6d5c9610ff09"
version = "0.5.13"

[[deps.Markdown]]
deps = ["Base64"]
uuid = "d6f4376e-aef5-505a-96c1-9c027394607a"

[[deps.MbedTLS_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "c8ffd9c3-330d-5841-b78e-0817d7145fa1"
version = "2.28.2+1"

[[deps.MicroCollections]]
deps = ["Accessors", "BangBang", "InitialValues"]
git-tree-sha1 = "44d32db644e84c75dab479f1bc15ee76a1a3618f"
uuid = "128add7d-3638-4c79-886c-908ea0c25c34"
version = "0.2.0"

[[deps.Missings]]
deps = ["DataAPI"]
git-tree-sha1 = "f66bdc5de519e8f8ae43bdc598782d35a25b1272"
uuid = "e1d29d7a-bbdc-5cf2-9ac0-f12de2c33e28"
version = "1.1.0"

[[deps.Mmap]]
uuid = "a63ad114-7e13-5084-954f-fe012c677804"

[[deps.MozillaCACerts_jll]]
uuid = "14a3606d-f60d-562e-9121-12d972cd8159"
version = "2023.1.10"

[[deps.NetworkOptions]]
uuid = "ca575930-c2e3-43a9-ace4-1e988b2c1908"
version = "1.2.0"

[[deps.OpenBLAS_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "Libdl"]
uuid = "4536629a-c528-5b80-bd46-f80d51c5b363"
version = "0.3.23+4"

[[deps.OrderedCollections]]
git-tree-sha1 = "dfdf5519f235516220579f949664f1bf44e741c5"
uuid = "bac558e1-5e72-5ebc-8fee-abe8a469f55d"
version = "1.6.3"

[[deps.Parameters]]
deps = ["OrderedCollections", "UnPack"]
git-tree-sha1 = "34c0e9ad262e5f7fc75b10a9952ca7692cfc5fbe"
uuid = "d96e819e-fc66-5662-9728-84c9c7592b0a"
version = "0.12.3"

[[deps.Parsers]]
deps = ["Dates", "PrecompileTools", "UUIDs"]
git-tree-sha1 = "8489905bcdbcfac64d1daa51ca07c0d8f0283821"
uuid = "69de0a69-1ddd-5017-9359-2bf0b02dc9f0"
version = "2.8.1"

[[deps.Pkg]]
deps = ["Artifacts", "Dates", "Downloads", "FileWatching", "LibGit2", "Libdl", "Logging", "Markdown", "Printf", "REPL", "Random", "SHA", "Serialization", "TOML", "Tar", "UUIDs", "p7zip_jll"]
uuid = "44cfe95a-1eb2-52ea-b672-e2afdf69b78f"
version = "1.10.0"

[[deps.PlotlyBase]]
deps = ["ColorSchemes", "Dates", "DelimitedFiles", "DocStringExtensions", "JSON", "LaTeXStrings", "Logging", "Parameters", "Pkg", "REPL", "Requires", "Statistics", "UUIDs"]
git-tree-sha1 = "56baf69781fc5e61607c3e46227ab17f7040ffa2"
uuid = "a03496cd-edff-5a9b-9e67-9cda94a718b5"
version = "0.8.19"

[[deps.PlutoPlotly]]
deps = ["AbstractPlutoDingetjes", "BaseDirs", "Colors", "Dates", "Downloads", "HypertextLiteral", "InteractiveUtils", "LaTeXStrings", "Markdown", "Pkg", "PlotlyBase", "Reexport", "TOML"]
git-tree-sha1 = "1ae939782a5ce9a004484eab5416411c7190d3ce"
uuid = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
version = "0.4.6"

    [deps.PlutoPlotly.extensions]
    PlotlyKaleidoExt = "PlotlyKaleido"
    UnitfulExt = "Unitful"

    [deps.PlutoPlotly.weakdeps]
    PlotlyKaleido = "f2990250-8cf9-495f-b13a-cce12b45703c"
    Unitful = "1986cc42-f94f-5a68-af5c-568840ba703d"

[[deps.PlutoUI]]
deps = ["AbstractPlutoDingetjes", "Base64", "ColorTypes", "Dates", "FixedPointNumbers", "Hyperscript", "HypertextLiteral", "IOCapture", "InteractiveUtils", "JSON", "Logging", "MIMEs", "Markdown", "Random", "Reexport", "URIs", "UUIDs"]
git-tree-sha1 = "71a22244e352aa8c5f0f2adde4150f62368a3f2e"
uuid = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
version = "0.7.58"

[[deps.PrecompileTools]]
deps = ["Preferences"]
git-tree-sha1 = "5aa36f7049a63a1528fe8f7c3f2113413ffd4e1f"
uuid = "aea7be01-6a6a-4083-8856-8a6e6704d82a"
version = "1.2.1"

[[deps.Preferences]]
deps = ["TOML"]
git-tree-sha1 = "9306f6085165d270f7e3db02af26a400d580f5c6"
uuid = "21216c6a-2e73-6563-6e65-726566657250"
version = "1.4.3"

[[deps.Printf]]
deps = ["Unicode"]
uuid = "de0858da-6303-5e67-8744-51eddeeeb8d7"

[[deps.REPL]]
deps = ["InteractiveUtils", "Markdown", "Sockets", "Unicode"]
uuid = "3fa0cd96-eef1-5676-8a61-b3b8758bbffb"

[[deps.Random]]
deps = ["SHA"]
uuid = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"

[[deps.Reexport]]
git-tree-sha1 = "45e428421666073eab6f2da5c9d310d99bb12f9b"
uuid = "189a3867-3050-52da-a836-e630ba90ab69"
version = "1.2.2"

[[deps.Requires]]
deps = ["UUIDs"]
git-tree-sha1 = "838a3a4188e2ded87a4f9f184b4b0d78a1e91cb7"
uuid = "ae029012-a4dd-5104-9daa-d747884805df"
version = "1.3.0"

[[deps.SHA]]
uuid = "ea8e919c-243c-51af-8825-aaa63cd721ce"
version = "0.7.0"

[[deps.Serialization]]
uuid = "9e88b42a-f829-5b0c-bbe9-9e923198166b"

[[deps.Setfield]]
deps = ["ConstructionBase", "Future", "MacroTools", "StaticArraysCore"]
git-tree-sha1 = "e2cc6d8c88613c05e1defb55170bf5ff211fbeac"
uuid = "efcf1570-3423-57d1-acb7-fd33fddbac46"
version = "1.1.1"

[[deps.Sockets]]
uuid = "6462fe0b-24de-5631-8697-dd941f90decc"

[[deps.SortingAlgorithms]]
deps = ["DataStructures"]
git-tree-sha1 = "66e0a8e672a0bdfca2c3f5937efb8538b9ddc085"
uuid = "a2af1166-a08f-5f64-846c-94a0d3cef48c"
version = "1.2.1"

[[deps.SparseArrays]]
deps = ["Libdl", "LinearAlgebra", "Random", "Serialization", "SuiteSparse_jll"]
uuid = "2f01184e-e22b-5df5-ae63-d93ebab69eaf"
version = "1.10.0"

[[deps.SplittablesBase]]
deps = ["Setfield", "Test"]
git-tree-sha1 = "e08a62abc517eb79667d0a29dc08a3b589516bb5"
uuid = "171d559e-b47b-412a-8079-5efa626c420e"
version = "0.1.15"

[[deps.StaticArrays]]
deps = ["LinearAlgebra", "PrecompileTools", "Random", "StaticArraysCore"]
git-tree-sha1 = "bf074c045d3d5ffd956fa0a461da38a44685d6b2"
uuid = "90137ffa-7385-5640-81b9-e52037218182"
version = "1.9.3"

    [deps.StaticArrays.extensions]
    StaticArraysChainRulesCoreExt = "ChainRulesCore"
    StaticArraysStatisticsExt = "Statistics"

    [deps.StaticArrays.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    Statistics = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"

[[deps.StaticArraysCore]]
git-tree-sha1 = "36b3d696ce6366023a0ea192b4cd442268995a0d"
uuid = "1e83bf80-4336-4d27-bf5d-d5a4f845583c"
version = "1.4.2"

[[deps.Statistics]]
deps = ["LinearAlgebra", "SparseArrays"]
uuid = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
version = "1.10.0"

[[deps.StatsAPI]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "1ff449ad350c9c4cbc756624d6f8a8c3ef56d3ed"
uuid = "82ae8749-77ed-4fe6-ae5f-f523153014b0"
version = "1.7.0"

[[deps.StatsBase]]
deps = ["DataAPI", "DataStructures", "LinearAlgebra", "LogExpFunctions", "Missings", "Printf", "Random", "SortingAlgorithms", "SparseArrays", "Statistics", "StatsAPI"]
git-tree-sha1 = "1d77abd07f617c4868c33d4f5b9e1dbb2643c9cf"
uuid = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
version = "0.34.2"

[[deps.SuiteSparse_jll]]
deps = ["Artifacts", "Libdl", "libblastrampoline_jll"]
uuid = "bea87d4a-7f5b-5778-9afe-8cc45184846c"
version = "7.2.1+1"

[[deps.TOML]]
deps = ["Dates"]
uuid = "fa267f1f-6049-4f14-aa54-33bafae1ed76"
version = "1.0.3"

[[deps.TableTraits]]
deps = ["IteratorInterfaceExtensions"]
git-tree-sha1 = "c06b2f539df1c6efa794486abfb6ed2022561a39"
uuid = "3783bdb8-4a98-5b6b-af9a-565f29a5fe9c"
version = "1.0.1"

[[deps.Tables]]
deps = ["DataAPI", "DataValueInterfaces", "IteratorInterfaceExtensions", "LinearAlgebra", "OrderedCollections", "TableTraits"]
git-tree-sha1 = "cb76cf677714c095e535e3501ac7954732aeea2d"
uuid = "bd369af6-aec1-5ad0-b16a-f7cc5008161c"
version = "1.11.1"

[[deps.Tar]]
deps = ["ArgTools", "SHA"]
uuid = "a4e569a6-e804-4fa4-b0f3-eef7a1d5b13e"
version = "1.10.0"

[[deps.TensorCore]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "1feb45f88d133a655e001435632f019a9a1bcdb6"
uuid = "62fd8b95-f654-4bbd-a8a5-9c27f68ccd50"
version = "0.1.1"

[[deps.Test]]
deps = ["InteractiveUtils", "Logging", "Random", "Serialization"]
uuid = "8dfed614-e22c-5e08-85e1-65c5234f0b40"

[[deps.Transducers]]
deps = ["Accessors", "Adapt", "ArgCheck", "BangBang", "Baselet", "CompositionsBase", "ConstructionBase", "DefineSingletons", "Distributed", "InitialValues", "Logging", "Markdown", "MicroCollections", "Requires", "SplittablesBase", "Tables"]
git-tree-sha1 = "47e516e2eabd0cf1304cd67839d9a85d52dd659d"
uuid = "28d57a85-8fef-5791-bfe6-a80928e7c999"
version = "0.4.81"

    [deps.Transducers.extensions]
    TransducersBlockArraysExt = "BlockArrays"
    TransducersDataFramesExt = "DataFrames"
    TransducersLazyArraysExt = "LazyArrays"
    TransducersOnlineStatsBaseExt = "OnlineStatsBase"
    TransducersReferenceablesExt = "Referenceables"

    [deps.Transducers.weakdeps]
    BlockArrays = "8e7c35d0-a365-5155-bbbb-fb81a777f24e"
    DataFrames = "a93c6f00-e57d-5684-b7b6-d8193f3e46c0"
    LazyArrays = "5078a376-72f3-5289-bfd5-ec5146d43c02"
    OnlineStatsBase = "925886fa-5bf2-5e8e-b522-a9147a512338"
    Referenceables = "42d2dcc6-99eb-4e98-b66c-637b7d73030e"

[[deps.Tricks]]
git-tree-sha1 = "eae1bb484cd63b36999ee58be2de6c178105112f"
uuid = "410a4b4d-49e4-4fbc-ab6d-cb71b17b3775"
version = "0.1.8"

[[deps.URIs]]
git-tree-sha1 = "67db6cc7b3821e19ebe75791a9dd19c9b1188f2b"
uuid = "5c2747f8-b7ea-4ff2-ba2e-563bfd36b1d4"
version = "1.5.1"

[[deps.UUIDs]]
deps = ["Random", "SHA"]
uuid = "cf7118a7-6976-5b1a-9a39-7adc72f591a4"

[[deps.UnPack]]
git-tree-sha1 = "387c1f73762231e86e0c9c5443ce3b4a0a9a0c2b"
uuid = "3a884ed6-31ef-47d7-9d2a-63182c4928ed"
version = "1.0.2"

[[deps.Unicode]]
uuid = "4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5"

[[deps.Zlib_jll]]
deps = ["Libdl"]
uuid = "83775a58-1f1d-513f-b197-d71354ab007a"
version = "1.2.13+1"

[[deps.libblastrampoline_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850b90-86db-534c-a0d3-1478176c7d93"
version = "5.8.0+1"

[[deps.nghttp2_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850ede-7688-5339-a07c-302acd2aaf8d"
version = "1.52.0+1"

[[deps.p7zip_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "3f19e933-33d8-53b3-aaab-bd5110c3b7a0"
version = "17.4.0+2"
"""

# ╔═╡ Cell order:
# ╟─516234a8-2748-11ed-35df-432eebaa5162
# ╠═5afad18b-1d87-450e-a0ff-8c1249d663ed
# ╟─65818e67-c146-4686-a9aa-d0859ef662fb
# ╠═c04c803c-cdca-4b8b-9c9d-e456ee677906
# ╠═563b6dbd-ce51-4904-b1cc-d766bd1fd1d6
# ╟─d5ac7c6f-9636-46d9-806f-34d6c8e4d4d5
# ╟─76489f21-677e-4c25-beaa-afaf2244cd94
# ╠═8dbc76fd-ac73-47ca-983e-0e90023390e3
# ╟─27d12c1c-ddb0-4bc1-af51-3388ff806705
# ╟─4d4baa61-b5bd-4bcf-a491-9a35a1695f0b
# ╠═cd139745-1877-43a2-97a0-3333e544cbd8
# ╠═0d0bbf62-b1ac-45f6-8a92-1e77b0709cb3
# ╟─cd79ce14-14a1-43c6-93e0-b4a786f7f9fb
# ╟─e0cc1ca1-595d-44e2-8612-261df9e2d327
# ╠═773ac9c5-c126-4e7d-b280-299adffcd840
# ╠═c71ae2cb-8492-4bbb-8fff-3f7c6d096563
# ╟─4f4551fe-54a9-4186-ab8f-3535dc2bf4c5
# ╠═870b7e41-7d1f-4af3-a145-8952a7fc8d78
# ╠═f3f05eb3-db68-44c8-806e-d09127276f4d
# ╠═79069fa0-56bc-4dca-9ccd-873c370bf9f8
# ╟─24efe9b4-9308-4ad1-8ef0-69f6f93407c0
# ╟─26fe0c28-8f0f-4cff-87fb-76f04fce1be1
# ╟─340ba72b-172a-4d92-99b2-17687ab511c7
# ╠═01f4268e-e947-4057-94a4-19d757be266d
# ╠═d00014fa-1539-4f42-ba63-15c7c9fecfde
# ╠═a2dbb1e2-2038-40ee-a2d9-8f5a594dd7a8
# ╠═95d18377-aef7-468c-8b2c-57f82bc7fe91
# ╠═321bdf5a-bff7-4181-986f-d3884ea96d27
# ╠═0fff8e1b-d0c2-49b8-93b4-8d1615c26690
# ╠═e689df6b-d6f0-4928-9212-a940aa00b0ef
# ╠═b03087e9-e15d-4563-bdae-4d9ba7d2cec6
# ╠═c1ff1bea-649c-4483-b4be-55134f0e8cb7
# ╠═037f1804-b24e-46e7-b2a8-6747e669db66
# ╠═c62cc32c-0d29-4ea2-8284-ac4c883df6db
# ╠═fb5601b0-06d4-43c4-81a6-23a4a8f29f00
# ╠═1d97325a-8b9a-438d-a5f9-e17638e64627
# ╠═7ae23e8e-d554-4d26-a08a-83dab507af13
# ╠═eb9ba23d-bee5-4bb1-b3e1-fe40d9f681dc
# ╠═618b5f35-2df5-4ffb-a34f-add542691080
# ╠═b15f1262-1acf-40e5-87a7-bc4b1b437a42
# ╠═6612f482-2f10-43fa-9b7b-2f0c6a94b8e8
# ╠═94b339bb-6e2d-422f-8043-615e8be9a217
# ╠═726af565-8905-4409-864f-a5c1b5767e09
# ╠═33f66659-1a87-4890-9137-dbc7776a19d8
# ╠═195d2a34-c44c-4088-8ec4-dece3107f16d
# ╠═80affd41-b5e6-4b9c-b827-4e3b39bd7767
# ╠═0899f37c-5def-4d15-8ca3-ebdec8e96b43
# ╠═6cf35193-dba5-4f78-a4ac-245dda7a0846
# ╠═96bd8d33-d4e8-45bf-9b75-43e8bda6fa07
# ╠═0adcbce8-2be5-48ef-af43-04815e10dc5c
# ╠═9a1b250f-b404-4db3-a4b7-4cd33b79d921
# ╠═324181f0-b890-4198-9b4b-c36547e6629a
# ╠═0f37ec0a-b737-478b-bf6a-027899250c4e
# ╠═f424edac-388d-4465-900f-9459d2a88f79
# ╠═4494cb61-ee2c-467b-9bf6-0afb59023e91
# ╠═143fff7d-0bb2-43b4-b810-53784fe848bd
# ╠═9be963b9-f3a1-4f92-8ff9-f5be75ed52f2
# ╠═113d2bc2-1f77-479f-86e5-a65b20672d7a
# ╠═5d2abde0-7128-41c3-bd1f-b6940492d1ae
# ╠═4e1c115a-4020-4a5a-a79a-56056892a953
# ╠═729197ce-2c27-467d-ba5f-47a1ecd539f2
# ╠═bb439641-30bd-495d-ba70-06b2e27efdbd
# ╠═77fde69f-2119-41eb-8993-a93b2c47ca7e
# ╠═eded8b72-70f4-4579-ba69-2eca409fa684
# ╠═1f7d77a6-d774-436d-a745-5a160cc15f2b
# ╠═502a7125-4460-4d39-be14-4852fb6d9ad2
# ╠═4b3604db-0c1b-4770-95b1-5f5bb34d071b
# ╠═39c96fc8-8259-46e3-88a0-a14eb6752b5c
# ╠═f5e52b2f-ea14-423d-8ca9-2ed68cd27c69
# ╠═3bf9e526-826d-42b8-84ee-75f1c7f79c69
# ╠═9d69687a-8df6-4e74-aa99-fbfcc84bcccf
# ╠═0f9080af-f166-4a78-a003-8df07f6c27d4
# ╟─00000000-0000-0000-0000-000000000001
# ╟─00000000-0000-0000-0000-000000000002
